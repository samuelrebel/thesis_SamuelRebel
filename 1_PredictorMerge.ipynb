{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28899,"status":"ok","timestamp":1749717148402,"user":{"displayName":"Samuel Rebel","userId":"12523763533102592733"},"user_tz":-120},"id":"ayVYY5rPBXRu","outputId":"c7cc7d50-6f34-4e00-8250-0035fff55557"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["#!/usr/bin/env python3\n","!pip install rasterio -q\n","import matplotlib.pyplot as plt\n","import geopandas as gpd\n","import pandas as pd\n","import rasterio\n","import numpy as np\n","from collections import defaultdict\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39170,"status":"ok","timestamp":1749717188716,"user":{"displayName":"Samuel Rebel","userId":"12523763533102592733"},"user_tz":-120},"id":"YrYPKfnRBhZj","outputId":"9a4fc981-4493-4381-f7ac-603ad1365c21"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 508190 HydroBASINS catchments for level 9.\n"]}],"source":["# ----------------------\n","# 1. Load all predictor data\n","# ----------------------\n","\n","continents = {\n","    \"Africa\": \"hybas_af\",\n","    \"Asia\": \"hybas_as\",\n","    \"SouthAmerica\": \"hybas_sa\",\n","    \"NorthAmerica\": \"hybas_na\",\n","    \"Europe\": \"hybas_eu\",\n","    \"Oceanie\": \"hybas_au\",\n","    \"Greenland\": \"hybas_gr\",\n","    \"Siberia\": \"hybas_si\",\n","    \"Arctic\": \"hybas_ar\"\n","}\n","\n","base_path = \"/content/drive/MyDrive/data_scriptie/predictor_data\"\n","continent_shapefiles = [\n","    f\"{base_path}/{continent}/{basename}_lev{str(9).zfill(2)}_v1c.shp\"\n","    for continent, basename in continents.items()\n","]\n","\n","gdfs = [gpd.read_file(path) for path in continent_shapefiles]\n","hydrobasins = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), crs=gdfs[0].crs)\n","\n","print(f\"Loaded {len(hydrobasins)} HydroBASINS catchments for level {level}.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NFA-nfczBw_D","executionInfo":{"status":"ok","timestamp":1749717364001,"user_tz":-120,"elapsed":109478,"user":{"displayName":"Samuel Rebel","userId":"12523763533102592733"}},"outputId":"17cf025a-c2a4-46e0-c19c-99a1fc3b819c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Merged HydroBASINS with HydroATLAS attributes.\n"]}],"source":["# ----------------------\n","# 2. Load HydroATLAS attributes and merge on HYBAS_ID\n","# ----------------------\n","atlas_file = \"/content/drive/MyDrive/data_scriptie/predictor_data/BasinAtlas/ShapeFiles/BasinATLAS_v10_lev09.shp\"\n","atlas = gpd.read_file(atlas_file)\n","common_cols = set(hydrobasins.columns).intersection(atlas.columns)\n","atlas_extra = atlas[[c for c in atlas.columns if c not in common_cols or c=='HYBAS_ID']]\n","gdf_basins = hydrobasins.merge(\n","    atlas_extra.drop(columns='geometry', errors='ignore'), on='HYBAS_ID', how='left'\n",")\n","\n","print(\"Merged HydroBASINS with HydroATLAS attributes.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uPa3K1ffDLnA"},"outputs":[],"source":["# ----------------------\n","# 3. Compute centroids in projected CRS then back to original\n","# ----------------------\n","projected_crs = \"EPSG:3857\"\n","gdf_proj = gdf_basins.to_crs(projected_crs)\n","gdf_proj['geometry'] = gdf_proj.geometry.centroid\n","gdf_centroids = gdf_proj.to_crs(gdf_basins.crs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ITsNm866hcR8","executionInfo":{"status":"ok","timestamp":1749386577193,"user_tz":-120,"elapsed":794652,"user":{"displayName":"Samuel Rebel","userId":"12523763533102592733"}},"outputId":"cc9e0f04-08a7-49d7-9c9f-374e2c254878"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sampled raster predictors.\n"]}],"source":["# ----------------------\n","# 4. Sample raster predictors on basin centroids\n","# ----------------------\n","predictor_paths = {\n","    'flow_direction':    \"/content/drive/MyDrive/data_scriptie/hyd_glo_dir_15s.tif\",\n","    'flow_length':       \"/content/drive/MyDrive/data_scriptie/hyd_glo_lup_15s.tif\",\n","    'flow_accumulation': \"/content/drive/MyDrive/data_scriptie/hyd_glo_acc_15s.tif\",\n","}\n","\n","def sample_all_predictors(gdf, predictor_paths):\n","    out = gdf.copy()\n","    for name, path in predictor_paths.items():\n","        with rasterio.open(path) as src:\n","            pts = out.to_crs(src.crs) if out.crs != src.crs else out\n","            coords = [(pt.x, pt.y) for pt in pts.geometry]\n","            vals = np.array([v[0] for v in src.sample(coords)])\n","            out[name] = np.where(vals == src.nodata, np.nan, vals)\n","    return out\n","\n","gdf_preds = sample_all_predictors(gdf_centroids, predictor_paths)\n","print(\"Sampled raster predictors.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jODNjmy9iz5J"},"outputs":[],"source":["# ----------------------\n","# 5. Build network maps and get_upstream_ids\n","# ----------------------\n","net_cols = ['NEXT_DOWN', 'NEXT_SINK', 'MAIN_BAS', 'UP_AREA', 'DIST_SINK', 'DIST_MAIN']\n","net_df = gdf_basins.set_index('HYBAS_ID')[net_cols]\n","down_map = net_df['NEXT_DOWN'].dropna().astype(int).to_dict()\n","up_map = defaultdict(list)\n","for src_id, dst_id in down_map.items():\n","    up_map[dst_id].append(src_id)\n","\n","def get_upstream_ids(basin_id):\n","    visited = set()\n","    stack = [basin_id]\n","    while stack:\n","        curr = stack.pop()\n","        for up in up_map.get(curr, []):\n","            if up not in visited:\n","                visited.add(up)\n","                stack.append(up)\n","    return visited"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B76mW5EElUiS","executionInfo":{"status":"ok","timestamp":1749386782969,"user_tz":-120,"elapsed":205170,"user":{"displayName":"Samuel Rebel","userId":"12523763533102592733"}},"outputId":"9cbfb7d7-7b37-44c4-b46f-e0dd46f039e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Computed upstream means.\n"]}],"source":["# ----------------------\n","# 6. Compute upstream-derived features\n","# ----------------------\n","\n","gdf_preds['upstream_count'] = gdf_preds['HYBAS_ID'].apply(lambda h: len(get_upstream_ids(h)))\n","for name in predictor_paths:\n","    col_local = name\n","    col_up = f'upstream_mean_{name}'\n","    means = []\n","    temp = gdf_preds.set_index('HYBAS_ID')\n","    for h in gdf_preds['HYBAS_ID']:\n","        ups = get_upstream_ids(h)\n","        means.append(temp.loc[list(ups), col_local].mean() if ups else np.nan)\n","    gdf_preds[col_up] = means\n","print(\"Computed upstream means.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ysEntN9pCCo"},"outputs":[],"source":["# ----------------------\n","# 7. Clean features: drop IDs, suffixes\n","# ----------------------\n","df = gdf_preds.drop(columns=['geometry'], errors='ignore')\n","id_prefixes = ['NEXT_DOWN','NEXT_SINK','MAIN_BAS','PFAF_ID','SORT','ORDER']\n","drop_id = [c for c in df.columns if any(c.startswith(pref) for pref in id_prefixes)]\n","drop_suf = [c for c in df.columns if c.endswith(('_x','_y'))]\n","df.drop(columns=drop_id + drop_suf, inplace=True, errors='ignore')\n","\n","\n","for col in ['UP_AREA','SUB_AREA','DIST_MAIN','flow_length','flow_accumulation']:\n","    if col in df.columns:\n","        df[col] = pd.to_numeric(df[col], errors='coerce')\n","\n","df.rename(columns={'UP_AREA':'up_area','SUB_AREA':'sub_area','DIST_MAIN':'dist_main'}, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FcwSVTitsC9d","executionInfo":{"status":"ok","timestamp":1749386783244,"user_tz":-120,"elapsed":32,"user":{"displayName":"Samuel Rebel","userId":"12523763533102592733"}},"outputId":"73de5bc2-aee0-4fd4-bd2f-1e4649c3321f"},"outputs":[{"output_type":"stream","name":"stdout","text":["   up_area  sub_area  dist_main\n","0     11.0      11.0        0.0\n","1    416.9     416.9        0.0\n","2    186.9     186.8        0.0\n","3    235.6     235.6        0.0\n","4      8.3       8.3        0.0\n","5    328.6     328.6        0.0\n","6      2.6       2.6        0.0\n","7    686.3     686.2        0.0\n","8     14.9      14.9        0.0\n","9   2925.8      17.1        0.0\n"]}],"source":["# ----------------------\n","# 8. Apply log1p transforms\n","# ----------------------\n","# Print raw values\n","print(df[['up_area','sub_area','dist_main']].head(10))\n","# Compute logs\n","df['up_area_log']   = np.log1p(df['up_area'])\n","df['sub_area_log']  = np.log1p(df['sub_area'])\n","df['dist_main_log'] = np.log1p(df['dist_main'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"of_aXqaX7IlX","executionInfo":{"status":"ok","timestamp":1749386909182,"user_tz":-120,"elapsed":125934,"user":{"displayName":"Samuel Rebel","userId":"12523763533102592733"}},"outputId":"3990eb2d-0370-4f58-93e0-19d404c17683"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saved features to /content/drive/MyDrive/data_scriptie/Output/features_final.csv.\n"]}],"source":["# ----------------------\n","# 9. Save final feature matrix\n","# ----------------------\n","up_cols = [c for c in df.columns if c.startswith(\"upstream_mean_\")]\n","for col in up_cols:\n","    df[col] = df[col].fillna(0)\n","\n","for col in ['sub_area','up_area', 'dist_main', 'flow_length', 'flow_accumulation']:\n","    if col in df.columns:\n","        df.drop(columns=[col], inplace=True)\n","\n","df = pd.get_dummies(df, columns=['flow_direction'], prefix='dir', dtype=int)\n","\n","atlas_extra = atlas[['HYBAS_ID', 'geometry']]\n","gdf_feats = pd.merge(df, atlas_extra, on='HYBAS_ID')\n","\n","out_file = '/content/drive/MyDrive/data_scriptie/Output/features_final.csv'\n","\n","gdf_feats.to_csv(\n","    out_file,\n","    index=False\n",")\n","print(f\"Saved features to {out_file}.\")"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"mount_file_id":"1Ffc_7ZSs-JGXBN0VlmrV25_HAE3sdqdA","authorship_tag":"ABX9TyNkWKvGv9wH8sS6T8uMMhLJ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}